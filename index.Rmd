--- 
title: "Un Recorrido por los Métodos Cuantitativos en Ciencias Sociales a bordo de R (edición preliminar)"
author: "Eduardo Bologna"
email: "eduardo.leon.bologna@unc.edu.ar"
date: "`r format(Sys.time(), '%B de %Y')`"
knit: "bookdown::render_book"
documentclass: book
link-citations: yes
colorlinks: yes
site: bookdown::bookdown_site
bibliography: bibliografia.bib
biblio-style: apalike
cover-image: imagenes/cover.jpg
github-repo: jcrodriguez1989/EstadisticaParaCienciasSocialesConR
always_allow_html: yes
---

```{r echo=FALSE}
source("depencias.R")
```

# Colaboradores     {.unnumbered}

## Edición en bookdown: {.unnumbered}  
### [Juan Cruz Rodriguez](https://jcrodriguez.rbind.io/)      {.unnumbered}

-------

Edición preliminar, en proceso de revisión por parte del:

## Comité Editorial:  {.unnumbered}  
####  Patricia Caro   {.unnumbered}
#### Adriana D´Amelio   {.unnumbered}
#### Maria Silvia Galibert   {.unnumbered}
#### Jorge Lorenzo   {.unnumbered}
#### María Marta Santillan   {.unnumbered}


```{r fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html')}
# tapa del libro (aparece solo el gitbook)
knitr::include_graphics("imagenes/cover.jpg", dpi = NA)
```

# Presentación {.unnumbered}

Este manual toma como punto de partida nuestro trabajo anterior _Métodos Estadísticos de Investigación_ [@bologna2018]. En lo didáctico, los cambios provienen de la experiencia acumulada luego de diez años de uso en carreras de Ciencias Sociales y Humanas de grado y posgrado. Los contenidos se han ajustado tomando en consideración el uso que se hace actualmente de la estadística en investigación social.

Los temas se presentan a partir de nociones básicas, para ilustrar la continuidad que hay entre las formas corrientes de razonar y su formalización. Se pone el acento en tres aspectos del uso de las técnicas: i) la comprensión del razonamiento que las sostiene, ii) los supuestos que autorizan su uso, y iii) la lectura e interpretación de los resultados. Al final del curso, se espera que los destinatarios de este manual puedan decidir ante qué interrogantes corresponde aplicar cada procedimiento, bajo qué condiciones se puede apelar a él y cómo se lee el resultado. Así es como se usa la estadística en el desempeño profesional o en investigación. El software elegido para aplicar las técnicas es R [@R] en entorno RStudio [@RStudioTeam2018], por un conjunto de razones que se explicitan más adelante. 

Las aplicaciones informáticas, que implican el modo de solicitar a R las operaciones de análisis están separadas de la discusión sobre contenidos y procedimientos, se decidió así por dos razones. La primera es que el manual puede ser usado en cursos que no incluyan las aplicaciones informáticas, sino que los objetivos se limiten a la comprensión e interpretación de procedimientos estadísticos, mientras que otros cursos pueden tener interés en el "cómo hacerlo", y necesitan algunas instrucciones para tener un manejo inicial de R. La segunda razón es que la elección de software de análisis de datos, del "paquete estadístico", suele depender de preferencias de los docentes y así el manual no queda atado exclusivamente a R.

Las fórmulas se consideran como una manera compacta de expresar relaciones entre conceptos y su uso para el cálculo está limitado, debido a que en la actualidad, las operaciones matemáticas se solicitan a paquetes informáticos especializados. Cada técnica que se describe tiene un fundamento teórico cuyo contenido matemático excede el interés de la mayoría de los destinatarios de este manual. Sin embargo, para interpretar adecuadamente un resultado o para decidir si determinada técnica puede aplicarse, es necesario conocer esos fundamentos; hemos intentado presentarlos apelando en la menor medida posible a conocimientos matemáticos previos y usando un mínimo de formalización.

El manual está organizado en tres partes que articulan los dos conjuntos de procedimientos por medio de los cuales la Estadística aporta a la construcción de conocimiento: el resumen de un conjunto grande de información y la extensión de las conclusiones que se observan en ciertos casos, a otros casos que no han sido observados. Estos dos conjuntos se denominan Estadística Descriptiva y Estadística Inferencial respectivamente. Las partes I y III de este manual corresponden a cada uno de ellos, mientras que la parte II es de transición, entre la certeza de la descripción de datos observados y la incertidumbre que acompaña a la generalización.

## Recorridos posibles {.unnumbered}
  
  - La parte I tiene suficiente autonomía como para constituir en sí misma, material bibliográfico para un curso de estadística descriptiva.

  - La parte III contiene lo inferencial, y requiere una introducción al concepto de probabilidad y de variabilidad muestral. Puede usarse la parte II como esta introducción o, según la naturaleza del curso, ésta puede presentarse de manera abreviada.

  - Cada capítulo termina con un apartado llamado "Hacerlo en R" que muestra como pedir a R las operaciones que se han visto en ese capítulo. Estos apartados puede ser obviados en los cursos que no incluyan las aplicaciones informáticas entre sus objetivos, o bien reemplazados por otros, según el software de análisis de datos que se elija.

# Introducción {.unnumbered}

Los procesos que involucran humanos, sean sociales, psicológicos, políticos, educativos, se caracterizan por su complejidad; es imposible predecir un resultado a partir del conocimiento de condiciones iniciales. Resultados electorales, reacciones de una persona en una situación dada, derroteros que sigue una protesta colectiva, a menudo discurren de modos que llamamos "impensados", haber conocido ciertos aspectos de estos procesos con antelación no nos habría permitido anticipar su devenir de manera exacta. Esto nos aleja de la ilusión determinística, o al menos de nuestra posibilidad de conocer una eventual determinación. Pero este rechazo al determinismo, no implica indeterminación. No es cierto que estemos en estado de ignorancia completa frente a la anticipación de procesos humanos, por el contrario, lo que tenemos es conocimiento parcial sobre algunos elementos que facilitan o hacen menos factibles ciertas consecuencias. Podemos aspirar a un conocimiento aproximado: entre el determinismo y la indeterminación, nuestro conocimiento se construye identificando condiciones bajo las cuales algunos hechos o procesos son más probables que otros. La manera formal de hacerlo es descomponiendo la diversidad en una parte que podemos explicar y otra que está fuera de nuestro alcance, llamamos variabilidad explicada a la primera y variabilidad aleatoria a la otra. La Estadística provee herramientas para trabajar con una forma de hacer investigación: el enfoque cuantitativo, que formula hipótesis sobre las explicaciones de hechos, fenómenos o procesos, observados y analiza cuán plausibles son, a la luz de los hechos. No permite alcanzar explicaciones completas, éstas siempre estarán afectadas por una componente de incertidumbre, propia de los procesos que involucran a seres humanos.

El cuantitativo no es el único enfoque que sirve para construir conocimiento validado y tampoco es incompatible con otras maneras de enfrentar la diversidad que nos rodea. Toda disciplina científica debe superar las creencias de "sentido común", esta ilusión de conocer, pero especialmente importante es eliminar los prejuicios en el campo de las ciencias que tratan con sujetos humanos, porque es allí donde más abundan y más daño pueden causar. Expresiones como "Esta persona es así porque de chico no lo tenían en cuenta", "No está dotado para la matemática", "Los sueños anuncian lo que va a pasar"; "Las mujeres tienen más sensibilidad que los hombres", "El electorado premia o castiga con el voto la gestión de gobierno", son, en el mejor de los casos, parciales y en el peor, falsas, provienen de creencias, de tradiciones, de voces populares transmitidas de una generación a la siguiente. La Estadística aporta a la investigación y con ella al descubrimiento de relaciones entre hechos, y a fundamentar esos descubrimientos. Lo hace con una mirada aguda y acotada, una mirada, que no aspira a ofrecer un conocimiento sobre alguna totalidad, sino sobre aspectos parciales del mundo que nos rodea. En esa limitación yace su fortaleza para poner en crisis la ilusión de conocer, en particular en Ciencias Sociales,  donde lo que observamos nos atañe de manera muy próxima, y no es raro confundir "lo que sucede" con lo que creemos, opinamos o suponemos sobre ello.

La presencia de Estadística en carreras de Ciencias Sociales, Humanas y de la Salud se justifica en primer lugar en que a menudo la investigación empírica usa información proveniente de colectivos, y se dispone de datos que requieren que se los resuma para poder interpretarlos, una vez obtenidas conclusiones acerca de esos casos que han sido observados, suele buscarse generalizar los resultados a otros casos, que no se observaron. Ambas operaciones ―resumir y generalizar―, requieren de la Estadística.

En segundo lugar, en las carreras de grado y posgrado de esas disciplinas hay varias materias que requieren que se conozca estadística básica para comprender los resultados de investigaciones que son material de estudio. Muy a menudo, esos resultados se expresan en lenguaje estadístico y se mencionan técnicas de análisis que deben conocerse para poder hacer críticas.

En tercer lugar, quienes se dediquen al ejercicio profesional aplicarán técnicas de intervención y de análisis en sus distintos campos de especialización, y estas técnicas están basadas en la teoría y en el estado del conocimiento en un momento dado. Así como luego de un tiempo de haber usado una droga puede descubrirse que no produce los efectos deseados, también vale esto para cualquier intervención profesional: una terapia, una estrategia didáctica, una recomendación de política pública, un asesoramiento de campaña política. La idea según la cual el conocimiento científico es revisable, se refiere a que en cualquier momento puede hallarse nueva evidencia que contradiga las convicciones presentes. Por cierto, no se trata de cualquier evidencia, sino de la que proviene de procedimientos cuidadosos de observación, registro, comparación, medición y análisis; en pocas palabras, de la investigación. Lo que sabemos y lo que aprendamos para desempeñarnos como profesionales es el conocimiento de que se dispone en este momento, y que está en continua reelaboración a través de la investigación. Una vez que quienes hoy están estudiando en la universidad terminen sus carreras y trabajen como profesionales, asistirán a cambios en el modo de intervenir, nuevos enfoques terapéuticos, formas de tratar con grupos, estrategias didácticas, modos de análisis de las desigualdades. Eso no se aprenderá en la Facultad, se aprenderá luego, manteniéndose actualizado, leyendo revistas científicas, asistiendo a congresos; en fin enterándose de cómo cambia el conocimiento y se revisan los saberes a partir de los resultados de la investigación. Y la investigación usa la estadística muy a menudo. Si no se puede leer un artículo científico porque no se entiende lo que dicen las cifras, solo se podrá tener una idea superficial del resultado, más grave aún; quien decida que solo leerá la información que no contenga cifras, se quedará con una pequeña parte de todo lo que se publica.

Esa limitación puede ser peligrosa: si no se entiende cómo se obtuvieron determinados resultados, no se los puede cuestionar, dudar de ellos, discutir procedimientos, porque parecerán ajenos.

La Estadística tiene rol  importante en la construcción, validación e interpretación de resultados de instrumentos de medición: tests de inteligencia, de desarrollo, escalas de actitudes, de preferencias, de opinión, de valores.

Por último, y aunque no parezca obvio, la Estadística nos sirve en la vida diaria. El ejercicio de nuestros derechos ciudadanos necesita que podamos darnos cuenta de lo que nos dicen las mediciones de audiencia, las consultoras políticas, los laboratorios de medicamentos, los indicadores nutricionales de lo que comemos, las estadísticas oficiales (nivel de desempleo, pobreza, inflación), entre otras fuentes de información. ¿A quiénes consultaron para decidir que un programa de televisión se levanta y otro se sostiene? ¿Cómo se hacen las encuestas que indican quién va a ganar las elecciones? ¿Qué implica que una técnica anticonceptiva sea eficaz en el 99% de los casos? Somos nosotros los que vemos la programación que se ofrece, somos nosotros los afectados por los resultados de una elección de autoridades, somos nosotros los que consumimos. Mucha de esa información usa estadísticas y hay parte del vocabulario que no es comprensible para todo el mundo pero que, por habernos habituado a escuchar, creemos conocer, ya que las palabras nos suenan familiares: el promedio, un porcentaje, que una diferencia sea significativa. Cada una de esas expresiones tiene un significado preciso; si no lo conocemos no podemos cuestionar las decisiones.

Que no lo conozcamos, que muchos prefieran no conocerlo y que haya quienes operan para que la Matemática -y por extensión la Estadística-, sean de difícil acceso a los estudiantes, no es por azar, es funcional a un modo de producción que requiere que en su mayoría, los ciudadanos puedan ser silenciados con argumentos que usan terminología críptica. Por esto, nuestra aspiración no se limita a transmitir un conjunto de técnicas de análisis de datos, sino a proveer herramientas que ayuden a mirar al mundo desde una posición más informada, y esperamos que eso implique también una posición más crítica y hasta quizás transformadora, que pueda poner en duda certezas establecidas.  

# Materiales y herramientas {.unnumbered}

## Materiales {.unnumbered}

A lo largo del manual se usan datos para ejemplificar procedimientos. Algunos de ellos son ficticios y han sido construidos especialmente para ilustrar algún aspecto de una distribución o del resultado de una técnica. Sin embargo, la mayoría de los ejemplos están realizados a partir de datos reales, provenientes de bases de datos de libre disponibilidad que hemos elegido teniendo en cuenta los intereses de los púbicos a que nos dirigimos. Estas bases son las que se presentan a continuación, en el repositorio (<!--¿GitHub?-->) se encuentra una descripción más detallada, así como los cuestionarios, las bases de datos correspondientes y los manuales de códigos para poner en correspondencia los nombres de las variables dados en la base, con los aspectos que se indagan de las unidades (hogares o personas).

### Encuesta Permanente de Hogares   {.unnumbered}

La EPH es un programa nacional de producción permanente de indicadores sociales cuyo objetivo es conocer las características socioeconómicas de la población. Es realizada en forma conjunta por el Instituto Nacional de Estadística y Censos (INDEC) y las Direcciones Provinciales de EstadÌstica (DPE) (@INDEC2003). Los datos se recogen por medio de dos cuestionarios; uno de ellos que pregunta por características del hogar y la vivienda y el otro por las personas individualmente.

### Encuesta Nacional de Factores de Riesgo {.unnumbered}  

La tercera ENFR es un estudio de corte transversal que permite vigilar la prevalencia de factores de riesgo de enfermedades no transmisibles y evaluar su evolución en comparación con las ediciones anteriores realizadas en los años 2005 y 2009. Es realizada conjuntamente entre el Instituto Nacional de Estadística y Censos (INDEC) y el Ministerio de Salud de la Nación de la República Argentina.

La encuesta tiene por objetivos: Conocer la distribución de los factores de riesgo en la población de 18 años y más. Estimar su prevalencia. Determinar el perfil de la población bajo riesgo a través de sus características sociodemográficas, socioeconómicas, educativas y del entorno familiar social @msal2013, @INDEC2013a.

### Latinobarómetro {.unnumbered}  

Latinobarómetro es un estudio de opinión pública que aplica anualmente alrededor de 20.000 entrevistas en 18 países de América Latina representando a más de 600 millones de habitantes.

Corporación Latinobarómetro es una ONG sin fines de lucro con sede en Santiago de Chile, única responsable de la producción y publicación de los datos.

La Corporación Latinobarómetro investiga el desarrollo de la democracia, la economía y la sociedad en su conjunto, usando indicadores de opinión pública que miden actitudes, valores y comportamientos. Los resultados son utilizados por los actores socio políticos de la región, actores internacionales, gubernamentales y medios de comunicación @Latinobarometro2010, @Latinobarometro2019.


### Encuesta Nacional sobre Prevalencias de Consumo de Sustancias Psicoactivas {.unnumbered}  

La encuesta produce datos sobre:  

• La cantidad de personas que declararon haber consumido sustancias psicoactivas en diferentes períodos de referencia (prevalencias) y su incidencia en relación con la población total. Las sustancias psicoactivas comprenden: las drogas legales o sociales (tabaco, bebidas alcohólicas), ilegales (marihuana, cocaína, pasta base, éxtasis, opiáceos y anestésicos, crack, alucinógenos, inhalables y otras drogas) y fármacos (estimulantes, tranquilizantes, anorexígenos);  

• Las características sociodemográficas, socioeconómicas, educativas y del entorno familiar social de la población de 16 a 65 años de edad que consume sustancias psicoactivas.  

El objetivo general de esta Encuesta fue contribuir a actualizar el sistema de información sobre el consumo de sustancias psicoactivas a nivel nacional y, de esa manera, al diseño de políticas públicas más eficaces, orientadas a mejorar las condiciones de salud de la población @Indec2008, @La2008.  

### Aplicación de la escala de Bayley {.unnumbered}  
La base provista contienen 454 casos de niñas y niños sometidos a evaluación por medio de las subescalas mental y motora de la escala de Bayley.  
La Escala Bayley de Desarrollo Infantil se aplica en personas de los 2 a los 36 meses de vida y ha sido adaptada a Córdoba a las edades de 0 a 24 meses @Rodriguez2005. Los dos primeros años de vida humana son momentos cruciales para inferir de manera temprana la evolución del desarrollo posterior (Papalia, 2000).   
La prueba, de origen norteamericano, consta de varias versiones desde su creación por su autora Nancy Bayley en 1933, existen revisiones de 1969, 1977 y 1993.
Las escalas de Bayley (EBDI) fueron publicadas en el año 1969. En España aparecieron estandarizadas en el año 1977, y la revisión americana (las Bayley Scale of Infant Development-II o BSID II) fueron publicadas en 1993, @Albers2007, @Ballot2017. El interés por disponer de unas escalas adecuadamente tipificadas como las Bayley creció a lo largo de la primera mitad del siglo XX, al ir aumentando el conocimiento de las capacidades infantiles, los cambios cerebrales durante la etapa posnatal y el estudio de cómo influyen los factores del medio, la familia y los individuos en el desarrollo temprano. Además, la proporción de prematuros ha ido aumentando considerablemente a lo largo de los años, con el consiguiente aumento de casos que nacen con el riesgo de presentar un retraso en el desarrollo.  
El objetivo inicial de las escalas fue medir la inteligencia a edades muy  tempranas. Los ítems que la componen están ordenados en una secuencia de dificultades que aumentan con la edad. En su forma actual, la EBDI permite medir el desarrollo mental, el desarrollo psicomotor y el comportamiento infantil.  
El nivel de desarrollo mental tiene 163 ítems que miden el desarrollo por medio de una serie de habilidades. El nivel del desarrollo motor se compone de 81 ítems que representan tanto habilidades motoras finas como gruesas, y se centra principalmente en el grado de control del cuerpo y la coordinación de los movimientos. La distinción entre la escala mental y motora se justifica porque se parte de que la preplanificación y el razonamiento son componentes  de las actividades mentales, pero no de las habilidades motoras. Por tanto, los ítems de las dos escalas son distintos, aunque en algunas ocasiones una misma habilidad este incluida en ambas escalas.  El registro comportamental  del sujeto complementa la información obtenida mediante las escalas mental y motora, proporciona una descripción del comportamiento infantil relativo al que se esperaría de cualquier persona que estuviera en el mismo estadio. 
Se dice que la escala de Bayley es una escala ecléctica, es decir los ítems no están agrupados por áreas, factores o subescalas con puntuaciones determinadas para cada una de ellas. No obstante, las Bayley II intenta incluir ítems de las cinco áreas que la guía americana para la evaluación de los candidatos de servicios especiales requiere que sean evaluadas: desarrollo físico, cognitivo, lingüístico, psicosocial y autoayuda. Sin embargo continúa sin haber medidas para cada una de estas áreas calificadas.  
Como  se menciono anteriormente en 1993 se publicó una revisión de las escalas originales. Las Bayley II mantienen la misma estructura y objetivos que la anterior, pero introducen algunos cambios como resultado de los trabajos de investigación sobre el desarrollo infantil en los últimos años y para mejorar la adecuación de los datos y el material a la población infantil. Uno de los cambios importantes es que se amplía la edad de evaluación desde el 1º mes hasta los 42 meses. Además se han incorporado ítems cuyo estudio han demostrado tener una adecuada fiabilidad y validez; así los 163 ítems de la escala mental y los 81 de la escala motora de Bayley  han pasado a ser 178 y 111 respectivamente en las Bayley II.  
Actualmente se utiliza la versión correspondiente a 1993. Como su autora lo indica, el objetivo principal de esta prueba es diagnosticar demoras del desarrollo y planificar estrategias de intervención.  Actualmente puede decirse que esta Escala constituye uno de los test más utilizados para evaluar el desarrollo neuroconductual durante los primeros meses de vida (Mayes et al., 1995), sin embargo es poco utilizada como herramienta de diagnóstico en nuestro medio. La misma está formada por tres Sub-Escalas:  

  -	Escala Mental: Evalúa memoria, habituación, resolución de problemas, conceptualización numérica temprana, generalización, clasificación, vocalizaciones, lenguaje y habilidades sociales. Es decir estima aspectos relacionados con el desarrollo cognitivo y  la capacidad de comunicación.  

  -	Escala Motora: Evalúa el control de los músculos gruesos y finos del cuerpo. Esto incluye movimientos asociados con rodar, girar, encogerse y estirarse, sentarse, pararse, caminar, correr y saltar. Se evalúa también la manipulación motora fina implicada en la prensión, adecuado uso de elementos de escritura e imitación de movimientos de las manos. En definitiva, evalúa el grado de coordinación corporal, así como habilidades motrices finas en manos y dedos.  

  -	Escala Comportamental: Evalúa aspectos cualitativos de la situación de examen como atención/alerta, orientación/interacción hacia la tarea, examinador y cuidador, regulación emocional y calidad del movimiento. De este modo permite analizar la naturaleza de las orientaciones sociales y objetivas hacia el entorno. La información de esta escala debe utilizarse para suplementar la información cuantitativa de las dos escalas anteriores.  

El desarrollo infantil implica un proceso de  continuos cambios, en el cual la persona va a experimentar diversas  trasformaciones y progresos. Estos cambios no sólo se advierten en el plano motor e intelectual (esto es, en su capacidad para dominar movimientos y razonar), sino que además se pueden apreciar  múltiples reorganizaciones  en el plano emocional (su capacidad para sentir) y  social (su capacidad para relacionarse con los demás).
La adquisición de estas competencias variará en función de la madurez neurofisiológica, de las características individuales, y de la riqueza y calidad de los estímulos de su entorno. De allí, la importancia de este período, pues sentará la base para el desarrollo y crecimiento óptimos.

La escala asigna puntajes brutos en función de la cantidad de pruebas exitosamente pasadas para una determinada edad. Posteriormente, estos puntajes brutos son transformados en índices mentales o motores que permiten ubicar a la persona evaluada en un nivel de desarrollo normal, levemente retrasado, significativamente retrasado o, si supera el normal, acelerado, para su edad.
Cada índice de desarrollo mental (IDM) y motor (IDP), tiene una distribución con una media de 100 y una desviación estándar de 15. Un puntaje de 100 o más en cualquier escala indica el nivel promedio de performance en una edad dada. Puntajes de 85 y 115 corresponden a 1 DS por debajo y por encima de la media y puntajes de 70 y 130 a 2 DS. En una distribución normal 2/3 de las personas evaluadas obtiene puntajes entre 85 y 115, 95% entre 70 y 130 y casi todas (99,9%) entre 55 y 145 (3 DS a cada lado de la media).  

### Adultos mayores {.unnumbered}
Urrutia informa


## Herramientas {.unnumbered}

### La elección de R {.unnumbered}

Para realizar análisis de datos existen numerosos programas informáticos, que se ocupan de los procesos computacionales, de manera que el usuario solo deba decidir qué procedimiento aplicar, cuidar que se cumplan las condiciones que hacen válido al procedimiento (los supuestos) y realizar una lectura correcta y completa del resultado que se obtiene, sin involucrarse con las operaciones de cálculo. Estos programas o "paquetes estadísticos" reúnen en un entorno único las operaciones más frecuentemente usadas por investigadores y analistas de datos y las ponen al alcance del usuario no especializado. Algunos de uso muy común son SPSS, SAS, INFOSTAT, STATA, STATISTICAL, etc. De la larga lista de opciones disponibles, este manual usa un software que se llama R, elección que se fundamenta en que R es varias cosas al mismo tiempo:

Es un software para análisis de datos: lo usan profesionales de la estadística, analistas de datos e investigadores de diversas disciplinas para extraer significado de información cuantitativa, para hacer descripciones e inferencias, visualización de datos y modelización predictiva.

Es un lenguaje de programación orientado a objetos, diseñado por estadísticos y para el uso en investigación cuantitativa: el análisis se hace escribiendo sentencias en este lenguaje, que provee objetos, operadores y funciones que hacen muy intuitivo el proceso de explorar, modelar y visualizar datos.

Es un ambiente para el análisis estadístico: en R hay funciones para prácticamente todo tipo de transformación de datos, de modelización y de representaciones gráficas que pueda hacer falta.

Es un proyecto de código abierto: esto significa no solo que se lo puede descargar y usar gratis, sino que el código es abierto y cualquiera puede inspeccionar o modificar las rutinas. Como sucede con otros proyectos de código abierto, como Linux, R ha mejorado sus códigos tras varios años de "muchos ojos mirando" y aportando soluciones. También como otros proyectos de código abierto, R tiene interfaces abiertas, por lo que se integra fácilmente a otras aplicaciones y sistemas.

Es una comunidad: R fue inicialmente desarrollado por Robert Gentleman y Ross Ihaka [@Ross1996], del Departamento de Estadística de la Universidad de Auckland, en 1993 y desde entonces el grupo que dirige el proyecto ha crecido y se ha difundido por el mundo. Además, miles de otras personas han contribuido con funcionalidades adicionales por medio del aporte de "paquetes" que utilizan los 2 millones de usuarios de todo el mundo. Como resultado, existe una intensa comunidad de usuarios de R on-line, con muchos sitios que ofrecen recursos para principiantes y para expertos. A esa comunidad se puede recurrir para consultas y para salvar dificultades, son muy activas y dispuestas a ayudar.



R integra programas llamados paquetes, que sirven para realizar análisis específicos. Los paquetes son rutinas que realizan conjuntos de operaciones especializadas, y una de las potencialidades de R es que diferentes investigadores pueden  desarrollar paquetes para determinados tipos de análisis y ponerlos a disposición de los demás usuarios. En la actualidad hay más de 10000 paquetes y el conjunto crece porque la comunidad R es muy activa y continuamente se hacen aportes. 

Actualmente, los principales medios de comunicación usan R para expresar datos de manera gráfica.

No solo cuenta con los métodos estándar sino que, debido a que los principales avances en procedimientos estadísticos se realizan en R, las técnicas más actualizadas están usualmente primero disponibles en R, a los desarrolladores de paquetes comerciales les lleva más tiempo poner las actualizaciones al alcance de los usuarios. Y los usuarios a menudo deben pagar por las actualizaciones.

Permite la reproducción de los análisis por parte de cualquiera que conozca el código que se aplicó, por lo que aporta una herramienta necesaria en los proyectos de ciencia abierta, en especial para la reproducibilidad de los resultados.

Hay sitios web (como [https://rdrr.io/snippets/](https://rdrr.io/snippets/), por ejemplo) que permiten ejecutar líneas de código on line, para situaciones en que no se cuenta con R instalado. También es posible crear una cuenta en  RStudio cloud (https://rstudio.cloud/) y trabajar con todos los archivos disponibles en la nube, sin necesidad de instalar localmente el software.  

Por estas razones, R es uno de los lenguajes de programación que más uso tiene y se está convirtiendo en la lingua franca del análisis de datos.

Como lenguaje, R tiene varias interfaces gráficas, que es el modo en que las personas puede interactuar con él. R es el motor y del mismo modo en que, para manejar un auto no hace falta saber cómo funciona el motor, también aquí será suficiente contar con un buen conjunto de comandos (volante, pedales…) para hacer uso de la potencia de ese motor, la interface provee esos comandos. De las interfaces existentes, hemos elegido RStudio [@RStudioTeam2018] que es un entorno de desarrollo integrado (IDE) de R para facilitar la edición de código que ofrece diversas herramientas para hacer muy accesible el uso de R por parte de quienes no se dedican a la programación, sino que son usuarios de procedimientos estadísticos.  
Una línea de tiempo sobre el desarrollo de R puede encontrar en https://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html  


### Instalación de R y RStudio {.unnumbered}

En [http://cran.r-project.org](http://cran.r-project.org) "Download R for [Linux, Mac o Windows]", y luego "install R for the first time". Una vez descargado, se instala siguiendo las instrucciones de las pantallas, aceptando las opciones por defecto que se ofrecen. 

Una vez que R está instalado, se debe sumar RStudio. El lugar de descarga es [http://www.rstudio.com/products/rstudio/download/](http://www.rstudio.com/products/rstudio/download/). Allí se elige la version gratis (free version) de RStudio Desktop y se baja hasta encontrar el sistema operativo y la version que corresponda a nuestro equipo. Luego se ejecuta el instalador de RStudio y se eligen las opciones por defecto. Cuando esté ya instalado, se accede por medio de RStudio; si al instalar R se creó un acceso directo a R en el escritorio, se lo puede eliminar. Al abrir RStudio, R es detectado automáticamente y desde allí operaremos.

### Los componentes de RStudio {.unnumbered}

Cuando abrimos RStudio, vamos a encontrar tres paneles, uno a la izquierda, más grande, y dos a la derecha. En "file" se solicita un nuevo script, que se abre a la izquierda y ahora quedan cuatro paneles: 


![Los cuatro paneles de RStudio](imagenes/entornoRStudio.png)

- Superior izquierdo es el script que se acaba de abrir, un documento editable en el que se escriben los comandos.

- Inferior izquierdo es la consola, donde se encuentra la ejecución de los comandos y, si corresponde, los resultados de operaciones solicitadas.

- Superior derecho es el entorno de trabajo, allí aparece cada uno de los objetos que se crean durante la sesión.

- Inferior derecho, cuatro pestañas con los directorios de trabajo, los paquetes instalados, la ayuda (cuando se pide), los gráficos que se hagan.


### Operaciones en el script {.unnumbered}

Antes de empezar a operar es necesario crear un lugar donde se alojarán los archivos que vamos a usar. Ese lugar, en R se llama "proyecto". Así, la primera acción será en File \rightarrow New Project \rightarrow New Directory \rightarrow New Project, darle un nombre y definir su ubicación en la computadora en que se trabaje. Si esa ubicación es una carpeta sincronizada (de drive o dropbox u otra) todos los archivos necesarios para trabajar en el proyecto estarán disponibles.  

El script es un editor de textos en que se escriben comandos y se ejecutan, ya sea con el botón “run” o con una combinación de teclas que, según la configuración puede ser Ctrl+R o Ctrl+Enter. Una vez escrita la instrucción, se solicita su ejecución y se obtiene el resultado.  
Los elementos que maneja R son objetos: un número, un vector, una base de datos, una tabla y muchos otros. Inicialmente, los que interesan a fin del análisis de datos son: vectores y matrices de datos.

#### Constante {.unnumbered}  
Un objeto numérico puede tener un valor fijo, si definimos a **x** como el número 3  

```{r echo=TRUE}
x <- 3
```

En el panel superior derecho aparece este objeto. El signo <- que define el objeto es equivalente a = que da la idea de asignar a **x** el valor 3.

Si se lo invoca (se lo llama es decir, se escribe su nombre), muestra su valor.


```{r echo=TRUE}
x
```

Las salidas de R, es decir los resultados que muestra, están antecedidos por un signo numeral (#) y cada elemento de los resultados lleva su numeración entre corchetes. Aquí el resultado es solo un número, por eso hay un [1] a la izquierda.  
Este objeto es un número, lo que puede saberse si se pregunta de qué clase es este objeto:

```{r echo=TRUE}
class(x)
```

Es numérico.

Si hubiésemos definido el objeto:

```{r echo=TRUE}
t <- "a"
class(t)
```


Es carácter, para que lo acepte como valor nuevo, se debe poner entre comillas; de lo contrario, si se escribe:

$t<-a$

Buscará ese objeto, que no ha sido definido antes y dará error. Esta cualidad se puede usar cuando los números codifican categorías, como cuando se usa 1 para varones y 2 para mujeres:

```{r echo=TRUE}
u <- "1"
class(u)
```

Allí se entiende al número como un código.

Otros tipos de objeto son lógicos

```{r echo=TRUE}
v <- TRUE
class(v)
```

El objeto **v** es lógico, esta clase de objeto puede tomar dos valores TRUE y FALSE.

Es posible transformar una clase de objeto en otra. Por ejemplo, si un valor numérico fue cargado como carácter, como el caso de **u** en el ejemplo anterior, se lo vuelve numérico pidiendo:

```{r echo=TRUE}
u <- as.numeric(u)
class(u)
```


Pero si intentáramos eso con **t**, el resultado falla, porque no se interpreta un valor numérico.


Cuando el objeto es un número, se puede operar simplemente con él

```{r echo=TRUE}
5 * x
```


Aquí no se creó ningún objeto nuevo, solo se hizo la operación y se mostró el resultado. Para crearlo, hace falta ponerle nombre:
```{r echo=TRUE}
y <- 5 * x
```

Y no veremos su valor hasta que no lo solicitemos

```{r echo=TRUE}
y
```

Suma resta, multiplicación y división se hacen con los signos que conocemos:

```{r echo=TRUE}
x + y
x - y
x * y
y / x
6 * x + 4 * y
```

Para elevar a una potencia se usa ^, por ejemplo, para hacer dos a la tercera potencia, es:

```{r echo=TRUE}
2^3
```


O $x$ (que está guardado con el valor 3) a la quinta potencia:

```{r echo=TRUE}
x^5
```

Las raíces son potencias fraccionarias, por lo que puede conseguirse la raíz cuadrada de $x$ así:

```{r echo=TRUE}
x^(1 / 2)
```

Pero como se usa a menudo, hay una función de biblioteca para eso:

```{r echo=TRUE}
sqrt(x)
```

Para raíces que no sean cuadradas, se debe usar la potencia fraccionaria. Para la raiz quinta de 24 es:

```{r echo=TRUE}
24^(1 / 5)
```

Un comando útil es el de redondeo. Si no queremos expresar la raiz de siete con seis decimales, sino solo con dos, se redondea a dos decimales:  

```{r echo=TRUE}
x <- sqrt(7)
x
round(x, 2)
## o todo de una sola vez:
round(sqrt(7), 2)
```


#### Vector {.unnumbered}  
Cuando se trabaja con variables, el conjunto de valores que asume es un objeto que se llama vector. Se lo genera con una letra c y paréntesis que indica concatenar valores. Por definición, un vector contiene elementos de la misma clase:

```{r echo=TRUE}
a <- c(1, 5, 8)
b <- c("x", "y", "z")
class(a)
class(b)
```

Si se intenta combinar diferentes clases de objeto, el vector tomará la clase con menos propiedades:

```{r echo=TRUE}
l <- c(1, 3, "a")
class(l)
```

Cuando pedimos que muestre los elementos de l:
```{r echo=TRUE}
l
```

Aparecen los números 1 y 3 entre comillas, lo que indica que los está tomando como caracteres, por lo que no podrá operar con ellos. Si lo intentamos por ejemplo, multiplicandolo por cinco, se obtiene un error:  

Error in 5 * c : non-numeric argument to binary operator  

Esto sucede porque los números fueron tratados como caracteres.  

Ejemplo: Los valores de PBI de cinco países son 10000, 3000, 7000, 4000 y 15000, se los puede concatenar así, definiendo el vector que los contiene con el nombre pib5:

```{r echo=TRUE}
pib5 <- c(10000, 3000, 7000, 4000, 15000)
```

Y se pueden hacer operaciones con él, por ejemplo, sumar sus valores

```{r echo=TRUE}
sum(pib5)
```

O sumarlos y dividir por 5, que va a dar el promedio:


```{r echo=TRUE}
sum(pib5) / 5
```


Pero para esto hay una función de biblioteca que calcula la media (promedio) directamente.

```{r echo=TRUE}
mean(pib5)
```

O definir un nuevo vector que consista en cada uno de ellos incrementado en un 10%:

```{r echo=TRUE}
pib5_10 <- 1.1 * pib5
pib5_10
```


Un vector puede crearse de este modo, concatenando varios valores, o bien como secuencia de números, por ejemplo creamos el vector $diez.pri$ como la secuencia de los números que van del 1 al 10, y pedimos que se muestre:  

```{r echo=TRUE}
diez.pri <- 1:10
diez.pri
```

Como puede verse, una vez que los objetos han sido creados, RStudio ofrece el autocompletado, eso vale también para comandos, por lo que no hace falta recordar con precisión el nombre de cada uno, se empieza a escribirlo y RStudio lo sugiere.  

Si se quiere que la secuencia tenga saltos de magnitud diferente a 1, el comando es `seq`, cuyos argumentos son: los números inicial y final de la secuencia y la amplitud del salto de cada valor al siguiente. Para ir del 1 al 10 de a 0.50:  

```{r echo=TRUE}
seq(1, 10, .5)
```

En este ejemplo, no creamos ningún objeto, solo solicitamos la secuencia para verla. El [16] que está debajo del [1] indica que el valor 8.5 ocupa el lugar 16 de la secuencia.   

El comando `rep`, repite un valor las veces que se solicite, repetir el valor 4, siete veces es:

```{r echo=TRUE}
rep(4, 7)
```

O el valor "perro", tres veces:

```{r echo=TRUE}
rep("perro", 3)
```

Estas maneras de generar secuencias pueden combinarse:

```{r echo=TRUE}
c(1:5, seq(1, 7, .8), rep(65, 4))
```

Notemos que hay un decimal (cero) en los enteros, eso es porque los números que componen el vector fueron interpretados como valores reales y no como enteros, como sucedió en el ejemplo anterior. Con que haya un solo número decimal, todos los componentes del vector son tratados como tales, a los enteros les corresponderá parte decimal igual a cero. Un vector siempre contiene elementos de la misma clase.  

Una clase de vector frecuente cuando se trata con variables cualitativas es el "factor", que está constituido por números que corresponden a etiquetas de valor. Por ejemplo, se define un vector como los códigos 1 y 2 para personas pertenecientes a los grupos experimental y control respectivamente y pedimos que se muestre:  
```{r echo=TRUE}
grupo <- c(1, 2)
class(grupo)
grupo
```
Es un vector numérico con valores 1 y 2. Luego indicamos que lo trate como un factor y volvemos a pedir su visualización:
```{r echo=TRUE}
grupo <- as.factor(grupo)
class(grupo)
grupo
```

Se trata de un factor y, si bien sus valores siguen siendo 1 y 2, ahora son llamados "niveles del factor". Los niveles pueden preguntarse explícitamente: 
```{r echo=TRUE}
levels(grupo)
```

Y también definirse, como etiquetas:
```{r echo=TRUE}
levels(grupo) <- c("experimental", "control")
```

Ahora éstos son los nuevos niveles:
```{r echo=TRUE}
levels(grupo)
```

Observemos la diferencia que esto tiene con haber evitado la codificación numérica y definir:
```{r echo=TRUE}
grupo_2 <- c("experimental", "control")
class(grupo_2)
grupo_2
grupo_2 <- as.factor(grupo_2)
levels(grupo_2)
grupo_2
```

Como el vector fue creado como de caracteres, sus valores se ordenan alfabéticamente. Cuando se muestra el vector, los niveles aparecen en el orden que elegimos, pero cuando se lo vuelve factor, se los ordena alfabéticamente. Eso es un problema que resolvemos evitando los vectores  de caracteres. Cuando deben usarse, se realiza una codificación numérica y luego se etiquetan los niveles.  

Si 10 personas han sido asignadas al grupo experimental y otras 10 al grupo control, el vector que representa su pertenencia puede ser:
```{r echo=TRUE}
pertenencia <- c(rep(1, 10), rep(2, 10))
pertenencia <- as.factor(pertenencia)
levels(pertenencia) <- c("experimental", "control")
```


Así como `class` indica de qué clase es un objeto, existen comandos para preguntar por características específicas de los objetos (como su clase, y otras) y obtener respuestas por sí o por no. Por ejemplo, si se pregunta si el valor de $x$ (recién definido) es un factor:

```{r echo=TRUE}
is.factor(x)
```
O si uno dividido cero es infinito:
```{r echo=TRUE}
is.infinite(1 / 0)
```

Este comando da un resultado de clase lógica, con FALSE y TRUE como posibilidades.  

Los corchetes, [], permiten seleccionar elementos de un vector. La lectura es que, del vector, se retienen los valores que cumplen con la condición que está dentro del corchete. Se define $z$, como la secuencia de 1 a 6 y luego a h como los elementos de z que sean menores a 5:
```{r echo=TRUE}
z <- c(1, 2, 3, 4, 5, 6)
h <- z[z < 5]
z
h
```

La longitud de un vector es el número de elementos que contiene, se solicita con el comando `length`:

```{r echo=TRUE}
length(z)
length(h)
```


En la variable pertenencia, los niveles son:
```{r echo=TRUE}
levels(pertenencia)
```
Mientras que los valores:
```{r echo=TRUE}
pertenencia
```

La longitud del vector es:
```{r echo=TRUE}
length(pertenencia)
```

#### Matriz de datos {.unnumbered}  
Cuando se combinan varios vectores, todos de la misma longitud, se construye un "data frame", una matriz de datos, cuyo formato más frecuente es que tenga los casos en las filas y variables en las columnas; cada columna es un vector que contiene los valores de cada variable.  
Por ejemplo, si tenemos 10 observaciones que corresponden a 7 varones y 3 mujeres, que son estudiantes de la universidad, y el vector que  representa el sexo de esas personas, con las categorías codificadas como 1 y 2, es:  
```{r echo=TRUE}
sexo <- c(rep(1, 7), rep(2, 3))
sexo <- as.factor(sexo)
levels(sexo) <- c("varones", "mujeres")
```

Se ha creado el vector *sexo* por medio de la concatenación de dos repeticiones, del 1 siete veces y del 2, tres veces. Luego se trató a ese vector como un factor y se etiquetaron sus niveles. El siguiente vector contiene las edades de las mismas personas:  
```{r echo=TRUE}
edad <- c(25, 28, 31, 20, 21, 22, 25, 28, 28, 28)
```

Entonces, se crea una matriz de datos con el comando:  

```{r echo=TRUE}
sexo_edad_estudiantes <- data.frame(sexo, edad)
```

En el panel superior derecho han aparecido los objetos que acaban de crearse. De este último se indica allí la cantidad de casos (observaciones) y de varables. Cuando se lo cliquea, se obtiene una vista en una ventana separada del script.  
La misma vista puede lograrse con el comando:  

```{r  echo=TRUE, eval = FALSE}
View(sexo_edad_estudiantes)
```

Para ver la primera parte de la matriz de datos, se usa el comando `head`:

```{r echo=TRUE}
head(sexo_edad_estudiantes)
```

Que, por defecto, muestra las seis primeras filas de la matriz.  

Esto que ha sido creado es un nuevo objeto, de clase:

```{r echo=TRUE}
class(sexo_edad_estudiantes)
```

Y cuyos atributos son:

```{r echo=TRUE}
attributes(sexo_edad_estudiantes)
```

Los nombres (names) son las denominaciones de las columnas (las variables), la clase es lo que solicitamos antes y row.names son los nombres de las filas, que por defecto coloca numerada consecutivamente. Cada uno de esos atributos está precedido por un signo pesos, ese es el modo de acceder a cada uno de ellos. Por ejemplo, para ver el vector que representa el sexo, pedimos:  
```{r echo=TRUE}
sexo_edad_estudiantes$sexo
```

El signo pesos separa el nombre de la matriz de datos del nombre de la variable: df$x quiere decir, la variable "x", perteneciente a la matriz "df".   
Los números entre corchetes (el [1] y [9] en este ejemplo) indican el número del primer elemento de esa fila. Podemos preguntar de qué clase es este vector:

```{r echo=TRUE}
class(sexo_edad_estudiantes$sexo)
```

Por defecto lo leyó como factor, con los dos niveles que se indican más arriba. A ellos se puede llegar directamente:  
```{r echo=TRUE}
levels(sexo_edad_estudiantes$sexo)
```

Y se los puede redefinir:  

```{r echo=TRUE}
levels(sexo_edad_estudiantes$sexo) <- c("femenino", "masculino")
```

Ahora la matriz se ve:

```{r  echo=TRUE, eval = FALSE}
View(sexo_edad_estudiantes)
```


El primer resumen útil de las variables de una matriz de datos es la tabla univariada. Para cada una de las dos variables, tenemos:  
```{r echo=TRUE}
table(sexo_edad_estudiantes$sexo)
table(sexo_edad_estudiantes$edad)
```

#### Lectura de una matriz de datos {.unnumbered}  
Es poco frecuente la creación de matrices de datos en R, salvo a fines de ejemplificación. Por el  contrario, a menudo es necesario leer un base que está guardada con un determinado formato (xls, ods, sav, sas, txt, csv, etc). El comando genérico es `read.table`, que requiere especificación sobre los simbolos que separan los campos y los decimales, si la primera fila lleva el nombre de las variables. Otros comandos más específicos son `read.csv`, `read.csv2`, el primero usa como separador por defecto ",", el segundo usa ";" y no necesitan que se indique si están los  nombres de las variables, porque por defecto los toman. A modo de ejemplo, leemos la base de la Encuesta Permanente de Hogares correspondiente al tercer trimestre de 2018. El archivo tiene frmato de texto (.txt) y se llama "usu_individual_T318.txt". Vamos a darle el nuevo nombre de eph.3.18:  
```{r echo=TRUE}
eph.3.18 <- read.table("bases/archivostxt/usu_individual_T318.txt",
  sep = ";", header = TRUE
)
```

Hemos indicado:  
- la ruta del archivo, dentro de nuestro directorio  
- que los campos están separados con un ";"  
- que la primera fila tiene lso nombres de las variables  

En el panel superior derecho aparece el nombre de este nuevo objeto con la cantidad de casos (filas) y de variables (columnas).  


#### Graficar {.unnumbered}
Para ver un ejemplo de gráfico básico, puede copiar y pegar el siguiente código en su script. Allí se define a $x$ como una secuencia de números que va de 1 a 10 en intervalos de 0.1. Luego se define $y$ como una función lineal de $x$ ($y=3-2*x$) a la que se agregan valores aleatorios provenientes de una distribución normal con media uno y desviación estándar cero. El tercer comando indica que se grafiquen las dos variables y les pone nombre a los ejes y al título del gráfico.

```{r echo=TRUE}
x <- seq(1, 10, .1)
y <- 3 - 2 * x + rnorm(91)
plot(x, y,
  xlab = "valor de x", ylab = "valor de y",
  main = "Tendencia lineal decreciente"
)
```


Cambie los coeficientes de la función lineal para probar los efectos en el gráfico.

### Instalación de paquetes {.unnumbered}

Cuando se descarga R y RStudio se cuenta con el sistema básico del lenguaje R. Las operaciones mencionadas en el apartado anterior y otras, están disponibles en esa base. Sin embargo, una gran cantidad de procedimientos están programados y ofrecidos como "paquetes", que sirven para tareas específicas. Su creación y desarrollo es parte de la potencialidad de R, porque son aportes de la comunidad que los diseña y los ofrece continuamente. En la actualidad hay más de 10000 paquetes en la CRAN (Comprehensive R Archive Network) aplicables a una gran diversidad de procedimientos.

La instalación de paquetes de R puede hacerse desde la línea de comando con la instrucción install.packages("") y el nombre del paquete entre comillas, también puede hacerse más directo, porque la IDE RStudio tiene, en el panel inferior derecho, una pestaña (la tercera) que dice *packages* y en ella, una opción *install* que abre una ventana para escribir (con autocompletado para los existentes) el nombre del paquete que se quiere instalar.  
A lo largo del curso y en la medida que sea necesario, se cargarán paquetes específicos.  
